{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Metric for Defect Prediction Based on LSTM with Pytorch",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNHfpQ643sl7xtRvQoaKoW5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eunjiWon/SoftwareDefectPredictionMetricUsingDeepLearning/blob/master/Metric_for_Defect_Prediction_Based_on_LSTM_with_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoJT8c3fZ3Jq",
        "colab_type": "text"
      },
      "source": [
        "# Get Test Set Cross-Entropy Using LSTM With Pytorch\n",
        "I refer this site: \n",
        "  [https://machinetalk.org/2019/02/08/text-generation-with-pytorch/](https://machinetalk.org/2019/02/08/text-generation-with-pytorch/)\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se5gEa-UoN0C",
        "colab_type": "text"
      },
      "source": [
        "### training_nutch_AllCommitAddLines.txt as a train set and DP_nutch_CommitFiles.txt as a test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4dINxC-1ho7",
        "colab_type": "code",
        "outputId": "af1f922c-78b8-42dc-8428-34656e9c2611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "from itertools import chain\n",
        "token_list = tokenize_data('/content/DP_nutch_CommitFiles.txt')\n",
        "print(token_list) # 각 라인별로 하나의 리스트로 묶인다. 전체를 하나의 리스트로 만들어야 함.\n",
        "\n",
        "def tokenize_data(filename):\n",
        "  with open(filename, \"r\") as file:\n",
        "    all_lines = file.readlines()\n",
        "  list_split_WS = [] \n",
        "  list_split_dot = []\n",
        "  for line in all_lines:\n",
        "    list_split_WS.append(line.split())\n",
        "  list_split_WS = list(chain.from_iterable(list_split_WS))\n",
        "  for token in list_split_WS:\n",
        "    list_split_dot.append(token.split('.'))\n",
        "  list_split_dot = list(chain.from_iterable(list_split_dot))\n",
        "  return list_split_dot\n",
        "\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['import', 'org', 'apache', 'hadoop', 'fs', 'FileStatus;', 'private', 'long', 'lastModified', '=', '0;', 'FileStatus', 'stat', '=', 'crawlDbPath', 'getFileSystem(config)', 'getFileStatus(crawlDbPath);', 'long', 'lastModified', '=', 'stat', 'getModificationTime();', 'synchronized', '(this)', '{', 'if', '(readers', '!=', 'null)', '{', 'if', '(this', 'lastModified', '==', 'lastModified)', '{', '//', 'CrawlDB', 'not', 'modified,', 're-use', 'readers', 'return;', '}', 'else', '{', '//', 'CrawlDB', 'modified,', 'close', 'and', 're-open', 'readers', 'closeReaders();', '}', '}', 'this', 'lastModified', '=', 'lastModified;', 'readers', '=', 'MapFileOutputFormat', 'getReaders(crawlDbPath,', 'config);', '}', 'import', 'org', 'apache', 'hadoop', 'fs', 'FileStatus;', 'private', 'long', 'lastModified', '=', '0;', 'public', 'void', 'openReaders()', 'throws', 'IOException', '{', 'Path', 'linkDbPath', '=', 'new', 'Path(directory,', 'LinkDb', 'CURRENT_NAME);', 'FileStatus', 'stat', '=', 'linkDbPath', 'getFileSystem(getConf())', 'getFileStatus(directory);', 'long', 'lastModified', '=', 'stat', 'getModificationTime();', 'synchronized', '(this)', '{', 'if', '(readers', '!=', 'null)', '{', 'if', '(this', 'lastModified', '==', 'lastModified)', '{', '//', 'CrawlDB', 'not', 'modified,', 're-use', 'readers', 'return;', '}', 'else', '{', '//', 'CrawlDB', 'modified,', 'close', 'and', 're-open', 'readers', 'close();', '}', '}', 'this', 'lastModified', '=', 'lastModified;', 'readers', '=', 'MapFileOutputFormat', 'getReaders(linkDbPath,', 'getConf());', '}', '}', 'openReaders();', 'output', 'append(\"\\\\n\");']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtGfml7zNJOs",
        "colab_type": "code",
        "outputId": "40ad4cc8-e793-440f-bde8-6c548aa7f952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "from argparse import Namespace\n",
        "from itertools import chain\n",
        "\n",
        "flags = Namespace(\n",
        "    train_file='/content/training_nutch_AllCommitAddLines.txt', # training_nutch_AllCommitAddLines.txt\n",
        "    test_file='/content/DP_nutch_CommitFiles.txt', # DP_nutch_CommitFiles.txt\n",
        "    seq_size=32, # evolve over t-time steps\n",
        "    batch_size=16,\n",
        "    embedding_size=64,\n",
        "    lstm_size=64, # LSTM hidden size\n",
        "    gradients_norm=5, # norm to clip gradients\n",
        ")\n",
        "\n",
        "def tokenize_data(filename):\n",
        "  with open(filename, \"r\") as file:\n",
        "    all_lines = file.readlines()\n",
        "  list_split_WS = [] \n",
        "  list_split_dot = []\n",
        "  for line in all_lines:\n",
        "    list_split_WS.append(line.split())\n",
        "  list_split_WS = list(chain.from_iterable(list_split_WS))\n",
        "  for token in list_split_WS:\n",
        "    list_split_dot.append(token.split('.'))\n",
        "  list_split_dot = list(chain.from_iterable(list_split_dot))\n",
        "  return list_split_dot\n",
        "\n",
        "def get_data_from_file(file, batch_size, seq_size):\n",
        "  text = tokenize_data(file)\n",
        "  # Create two dictionaries, one to convert words into integers indices, \n",
        "  # and the other one to convert integer indices back to word tokens\n",
        "  word_counts = Counter(text)\n",
        "  sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "  int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "  vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "  n_vocab = len(int_to_vocab)\n",
        "  print('Vocabulary size', n_vocab)\n",
        "  \n",
        "  # Covert word tokens into integer indices. \n",
        "  # These will be the input to the network\n",
        "  # We will train a mini-batch each iteration \n",
        "  # so we split the data into batches evenly. \n",
        "  # Chopping out the last uneven batch\n",
        "  int_text = [vocab_to_int[w] for w in text]\n",
        "  num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "  in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "  \n",
        "  # In next generation problem, \n",
        "  # the target of each input word will be its consecutive wold,\n",
        "  # so we just shift the whole input data to the left by one step\n",
        "  out_text = np.zeros_like(in_text)\n",
        "  out_text[:-1] = in_text[1:] # in_text의 두번째 부터 out_text의 처음으로 복사\n",
        "  out_text[-1] = in_text[0] # in_text의 처음을 out_text의 마지막으로 복사\n",
        "  in_text = np.reshape(in_text, (batch_size, -1))\n",
        "  out_text = np.reshape(out_text, (batch_size, -1))\n",
        "  # print(in_text[:10][:10]) # top and left of matrix\n",
        "  # print(out_text[:10][:10]) # top and left of matrix\n",
        "  return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text\n",
        "  \n",
        "def get_batches(in_text, out_text, batch_size, seq_size):\n",
        "  num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
        "  for i in range(0, num_batches * seq_size, seq_size):\n",
        "      yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]\n",
        "\n",
        "class RNNModule(nn.Module):\n",
        "  def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
        "    super(RNNModule, self).__init__()\n",
        "    self.seq_size = seq_size\n",
        "    self.lstm_size = lstm_size\n",
        "    self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "    self.lstm = nn.LSTM(embedding_size, lstm_size, batch_first=True)\n",
        "    self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "\n",
        "  # Take an input sequence and the previous states (hidden states) and produce the output together with states of the currents timestamp\n",
        "  def forward(self, x, prev_state):\n",
        "    embed = self.embedding(x)\n",
        "    output, state = self.lstm(embed, prev_state)\n",
        "    logits = self.dense(output)\n",
        "    return logits, state # why return state variable?\n",
        "  \n",
        "  # Define one more method to help us set states to zero because we need to reset states at the beginning of every epoch.\n",
        "  def zero_state(self, batch_size):\n",
        "    return (torch.zeros(1, batch_size, self.lstm_size), # hidden state (the short-term memory)\n",
        "            torch.zeros(1, batch_size, self.lstm_size)) # cell state (the long-term memory)\n",
        "\n",
        "def get_loss_and_train_op(net, lr=0.001):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  return criterion, optimizer\n",
        "  # gradient clipping doesn't apply here!\n",
        "\n",
        "def train(in_text, out_text, flags, net, device, criterion, optimizer, e):\n",
        "  batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
        "  state_h, state_c = net.zero_state(flags.batch_size)\n",
        "  # Transfer data to GPU\n",
        "  state_h = state_h.to(device)\n",
        "  state_c = state_c.to(device)\n",
        "\n",
        "  iteration = 0\n",
        "  for x, y in batches: # x is in_text and y is out_text\n",
        "    iteration += 1\n",
        "    # Tell it we are in training mode\n",
        "    net.train()\n",
        "    # Reset all gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Transfer data to GPU\n",
        "    x = torch.tensor(x).to(device)\n",
        "    y = torch.tensor(y).to(device)\n",
        "    logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "    loss = criterion(logits.transpose(1, 2), y) # why we transpose the logits?\n",
        "    # Avoid autograd which is given by Pytorch to keep track of the tensor's flow to perform back-propagation.\n",
        "    state_h = state_h.detach()\n",
        "    state_c = state_c.detach()\n",
        "    loss_value = loss.item() # this loss is cross-entropy which is thing I want!!! \n",
        "    # Perform back-propagation\n",
        "    loss.backward()\n",
        "    # Gradient clipping\n",
        "    _ = torch.nn.utils.clip_grad_norm_(net.parameters(), flags.gradients_norm)\n",
        "    # Update the network's parameters\n",
        "    optimizer.step() # the number of parameters update is batch-size * epoch\n",
        "  # Print the loss value and have the model generate some text for us during training\n",
        "  print('Epoch: {}/{}'.format(e, 200), 'Loss (C.E): {}'.format(loss_value)) # here, we just print the size of epoch \n",
        "\n",
        "def test(test_in_text, test_out_text, net, device, criterion):\n",
        "  net.eval() # Tell it we are in evaluation mode\n",
        "  # 학습된 parameters을 이용하고 hidden and cell states는 초기화 시켜야함. \n",
        "  state_h, state_c = net.zero_state(1) #\n",
        "  state_h = state_h.to(device) #\n",
        "  state_c = state_c.to(device) #\n",
        "  # Transfer data to GPU\n",
        "  x = torch.tensor(test_in_text).to(device)\n",
        "  y = torch.tensor(test_out_text).to(device)\n",
        "  logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "  loss = criterion(logits.transpose(1, 2), y) # why we transpose the logits?\n",
        "  loss_value = loss.item() # this loss is cross-entropy which is thing I want!!! \n",
        "  print(\"test set loss value (C.E.): \", loss_value)\n",
        "\n",
        "def main():\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
        "      flags.train_file, flags.batch_size, flags.seq_size)\n",
        "  test_int_to_vocab, test_vocab_to_int, test_n_vocab, test_in_text, test_out_text = get_data_from_file(\n",
        "      flags.test_file, 1, flags.seq_size)\n",
        "  net = RNNModule(n_vocab, flags.seq_size, flags.embedding_size, flags.lstm_size)\n",
        "  net = net.to(device)\n",
        "  criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
        "  \n",
        "  # Training\n",
        "  for e in range(50):\n",
        "    train(in_text, out_text, flags, net, device, criterion, optimizer, e)\n",
        "  \n",
        "  # Test\n",
        "  test(test_in_text, test_out_text, net, device, criterion)\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size 65486\n",
            "Vocabulary size 64\n",
            "Epoch: 0/200 Loss (C.E): 4.7521891593933105\n",
            "Epoch: 1/200 Loss (C.E): 3.5346152782440186\n",
            "Epoch: 2/200 Loss (C.E): 2.9828176498413086\n",
            "Epoch: 3/200 Loss (C.E): 2.640777826309204\n",
            "Epoch: 4/200 Loss (C.E): 2.455153703689575\n",
            "Epoch: 5/200 Loss (C.E): 2.2705812454223633\n",
            "Epoch: 6/200 Loss (C.E): 2.161515712738037\n",
            "Epoch: 7/200 Loss (C.E): 2.1456027030944824\n",
            "Epoch: 8/200 Loss (C.E): 2.0428504943847656\n",
            "Epoch: 9/200 Loss (C.E): 2.0305047035217285\n",
            "Epoch: 10/200 Loss (C.E): 2.05074143409729\n",
            "Epoch: 11/200 Loss (C.E): 2.0234522819519043\n",
            "Epoch: 12/200 Loss (C.E): 2.001739263534546\n",
            "Epoch: 13/200 Loss (C.E): 2.0418975353240967\n",
            "Epoch: 14/200 Loss (C.E): 1.9405914545059204\n",
            "Epoch: 15/200 Loss (C.E): 1.9398503303527832\n",
            "Epoch: 16/200 Loss (C.E): 1.9209319353103638\n",
            "Epoch: 17/200 Loss (C.E): 1.8933955430984497\n",
            "Epoch: 18/200 Loss (C.E): 1.8901100158691406\n",
            "Epoch: 19/200 Loss (C.E): 1.9184101819992065\n",
            "Epoch: 20/200 Loss (C.E): 1.804491400718689\n",
            "Epoch: 21/200 Loss (C.E): 1.818009376525879\n",
            "Epoch: 22/200 Loss (C.E): 1.8539870977401733\n",
            "Epoch: 23/200 Loss (C.E): 1.8349270820617676\n",
            "Epoch: 24/200 Loss (C.E): 1.846343994140625\n",
            "Epoch: 25/200 Loss (C.E): 1.7763324975967407\n",
            "Epoch: 26/200 Loss (C.E): 1.8133982419967651\n",
            "Epoch: 27/200 Loss (C.E): 1.8445802927017212\n",
            "Epoch: 28/200 Loss (C.E): 1.7412985563278198\n",
            "Epoch: 29/200 Loss (C.E): 1.8170558214187622\n",
            "Epoch: 30/200 Loss (C.E): 1.7419992685317993\n",
            "Epoch: 31/200 Loss (C.E): 1.7882150411605835\n",
            "Epoch: 32/200 Loss (C.E): 1.863189458847046\n",
            "Epoch: 33/200 Loss (C.E): 1.9510700702667236\n",
            "Epoch: 34/200 Loss (C.E): 1.8605878353118896\n",
            "Epoch: 35/200 Loss (C.E): 1.803361177444458\n",
            "Epoch: 36/200 Loss (C.E): 1.7768745422363281\n",
            "Epoch: 37/200 Loss (C.E): 1.882223129272461\n",
            "Epoch: 38/200 Loss (C.E): 1.8276716470718384\n",
            "Epoch: 39/200 Loss (C.E): 1.854633092880249\n",
            "Epoch: 40/200 Loss (C.E): 1.8885784149169922\n",
            "Epoch: 41/200 Loss (C.E): 1.8232173919677734\n",
            "Epoch: 42/200 Loss (C.E): 1.7952936887741089\n",
            "Epoch: 43/200 Loss (C.E): 1.7668771743774414\n",
            "Epoch: 44/200 Loss (C.E): 1.7856017351150513\n",
            "Epoch: 45/200 Loss (C.E): 1.7332602739334106\n",
            "Epoch: 46/200 Loss (C.E): 1.8068571090698242\n",
            "Epoch: 47/200 Loss (C.E): 1.7585655450820923\n",
            "Epoch: 48/200 Loss (C.E): 1.8152210712432861\n",
            "Epoch: 49/200 Loss (C.E): 1.7340843677520752\n",
            "test set loss value (C.E.):  13.306751251220703\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}