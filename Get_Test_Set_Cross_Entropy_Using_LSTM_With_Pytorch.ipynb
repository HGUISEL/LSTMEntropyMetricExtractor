{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Get Test Set Cross-Entropy Using LSTM With Pytorch",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPHo2ZCuU8BZeIEivpekcGf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eunjiWon/SoftwareDefectPredictionMetricUsingDeepLearning/blob/master/Get_Test_Set_Cross_Entropy_Using_LSTM_With_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoJT8c3fZ3Jq",
        "colab_type": "text"
      },
      "source": [
        "# Get Test Set Cross-Entropy Using LSTM With Pytorch\n",
        "I refer this site: \n",
        "  [https://machinetalk.org/2019/02/08/text-generation-with-pytorch/](https://machinetalk.org/2019/02/08/text-generation-with-pytorch/)\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se5gEa-UoN0C",
        "colab_type": "text"
      },
      "source": [
        "### oliver_ch1.txt as a train set and trump.txt as a test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtGfml7zNJOs",
        "colab_type": "code",
        "outputId": "5c418d17-f76e-47b0-ec32-1cc6d97d0ea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "from argparse import Namespace\n",
        "\n",
        "flags = Namespace(\n",
        "    train_file='/content/oliver_ch1.txt',\n",
        "    test_file='/content/trump.txt',\n",
        "    seq_size=32,\n",
        "    batch_size=16,\n",
        "    embedding_size=64,\n",
        "    lstm_size=64, # LSTM hidden size\n",
        "    gradients_norm=5, # norm to clip gradients\n",
        "    initial_words=['I', 'am'], # Initial words to start prediction from\n",
        "    predict_top_k=5, # top k results ro sample word from\n",
        "    # checkpoint_path='checkpoint',\n",
        ")\n",
        "\n",
        "def get_data_from_file(train_file, batch_size, seq_size):\n",
        "    with open(train_file, 'r') as f:\n",
        "        text = f.read()\n",
        "    text = text.split()\n",
        "    # Create two dictionaries, one to convert words into integers indices, \n",
        "    # and the other one to convert integer indices back to word tokens\n",
        "    word_counts = Counter(text)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "    n_vocab = len(int_to_vocab)\n",
        "    print('Vocabulary size', n_vocab)\n",
        "    \n",
        "    # Covert word tokens into integer indices. \n",
        "    # These will be the input to the network\n",
        "    # We will train a mini-batch each iteration \n",
        "    # so we split the data into batches evenly. \n",
        "    # Chopping out the last uneven batch\n",
        "    int_text = [vocab_to_int[w] for w in text]\n",
        "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "    \n",
        "    # In next generation problem, \n",
        "    # the target of each input word will be its consecutive wold,\n",
        "    # so we just shift the whole input data to the left by one step\n",
        "    out_text = np.zeros_like(in_text)\n",
        "    out_text[:-1] = in_text[1:] # in_text의 두번째 부터 out_text의 처음으로 복사\n",
        "    out_text[-1] = in_text[0] # in_text의 처음을 out_text의 마지막으로 복사\n",
        "    in_text = np.reshape(in_text, (batch_size, -1))\n",
        "    out_text = np.reshape(out_text, (batch_size, -1))\n",
        "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text\n",
        "  \n",
        "def get_batches(in_text, out_text, batch_size, seq_size):\n",
        "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
        "    for i in range(0, num_batches * seq_size, seq_size):\n",
        "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]\n",
        "\n",
        "class RNNModule(nn.Module):\n",
        "  def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
        "    super(RNNModule, self).__init__()\n",
        "    self.seq_size = seq_size\n",
        "    self.lstm_size = lstm_size\n",
        "    self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "    self.lstm = nn.LSTM(embedding_size, lstm_size, batch_first=True)\n",
        "    self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "\n",
        "  # Take an input sequence and the previous states (hidden states) and produce the output together with states of the currents timestamp\n",
        "  def forward(self, x, prev_state):\n",
        "    embed = self.embedding(x)\n",
        "    output, state = self.lstm(embed, prev_state)\n",
        "    logits = self.dense(output)\n",
        "    return logits, state # why return state variable?\n",
        "  \n",
        "  # Define one more method to help us set states to zero because we need to reset states at the beginning of every epoch.\n",
        "  def zero_state(self, batch_size):\n",
        "    return (torch.zeros(1, batch_size, self.lstm_size), # hidden state (the short-term memory)\n",
        "            torch.zeros(1, batch_size, self.lstm_size)) # cell state (the long-term memory)\n",
        "\n",
        "def get_loss_and_train_op(net, lr=0.001):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  return criterion, optimizer\n",
        "  # gradient clipping doesn't apply here!\n",
        "\n",
        "def main():\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
        "      flags.train_file, flags.batch_size, flags.seq_size)\n",
        "  test_int_to_vocab, test_vocab_to_int, test_n_vocab, test_in_text, test_out_text = get_data_from_file(\n",
        "      flags.test_file, 1, flags.seq_size)\n",
        "  net = RNNModule(n_vocab, flags.seq_size, flags.embedding_size, flags.lstm_size)\n",
        "  net = net.to(device)\n",
        "  criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
        "  iteration = 0\n",
        "  \n",
        "  # Train\n",
        "  # for each epoch, we will loop through the batches to compute loss valuse and update network's parameters.\n",
        "  for e in range(50):\n",
        "    batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
        "    state_h, state_c = net.zero_state(flags.batch_size)\n",
        "    \n",
        "    # Transfer data to GPU\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    for x, y in batches: # x is in_text and y is out_text\n",
        "      iteration += 1\n",
        "      \n",
        "      # Tell it we are in training mode\n",
        "      net.train()\n",
        "\n",
        "      # Reset all gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Transfer data to GPU\n",
        "      x = torch.tensor(x).to(device)\n",
        "      y = torch.tensor(y).to(device)\n",
        "\n",
        "      logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "      loss = criterion(logits.transpose(1, 2), y) # why we transpose the logits?\n",
        "      \n",
        "      # Avoid autograd which is given by Pytorch to keep track of the tensor's flow to perform back-propagation.\n",
        "      state_h = state_h.detach()\n",
        "      state_c = state_c.detach()\n",
        "\n",
        "      loss_value = loss.item() # this loss is cross-entropy which is thing I want!!! \n",
        "      \n",
        "      # Perform back-propagation\n",
        "      loss.backward()\n",
        "      \n",
        "      # Gradient clipping\n",
        "      _ = torch.nn.utils.clip_grad_norm_(net.parameters(), flags.gradients_norm)\n",
        "\n",
        "\n",
        "      # Update the network's parameters\n",
        "      optimizer.step()\n",
        "\n",
        "      # Print the loss value and have the model generate some text for us during training\n",
        "      if iteration % 100 == 0:\n",
        "        print('Epoch: {}/{}'.format(e, 200), 'Iteration: {}'.format(iteration), 'Loss (C.E): {}'.format(loss_value))\n",
        "\n",
        "      # if iteration % 1000 == 0:\n",
        "      #   predict(device, net, flags.initial_words, n_vocab, vocab_to_int, int_to_vocab, top_k=5)\n",
        "      #   torch.save(net.state_dict(), 'checkpoint_pt/model-{}.pth'.format(iteration))\n",
        "  \n",
        "  # Test\n",
        "  net.eval() # Tell it we are in evaluation mode\n",
        "  # 학습된 parameters을 이용하고 hidden and cell states는 초기화 시켜야함. \n",
        "  state_h, state_c = net.zero_state(1) #\n",
        "  state_h = state_h.to(device) #\n",
        "  state_c = state_c.to(device) #\n",
        "  # Transfer data to GPU\n",
        "  x = torch.tensor(test_in_text).to(device)\n",
        "  y = torch.tensor(test_out_text).to(device)\n",
        "  logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "  loss = criterion(logits.transpose(1, 2), y) # why we transpose the logits?\n",
        "  loss_value = loss.item() # this loss is cross-entropy which is thing I want!!! \n",
        "  print(\"test set loss value (C.E): \", loss_value)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size 4694\n",
            "Vocabulary size 1421\n",
            "Epoch: 3/200 Iteration: 100 Loss (C.E): 5.773972034454346\n",
            "Epoch: 6/200 Iteration: 200 Loss (C.E): 4.309805870056152\n",
            "Epoch: 9/200 Iteration: 300 Loss (C.E): 3.118530511856079\n",
            "Epoch: 12/200 Iteration: 400 Loss (C.E): 2.314215660095215\n",
            "Epoch: 16/200 Iteration: 500 Loss (C.E): 1.917165756225586\n",
            "Epoch: 19/200 Iteration: 600 Loss (C.E): 1.917675256729126\n",
            "Epoch: 22/200 Iteration: 700 Loss (C.E): 1.3184051513671875\n",
            "Epoch: 25/200 Iteration: 800 Loss (C.E): 1.0215789079666138\n",
            "Epoch: 29/200 Iteration: 900 Loss (C.E): 0.7629404664039612\n",
            "Epoch: 32/200 Iteration: 1000 Loss (C.E): 0.7583263516426086\n",
            "Epoch: 35/200 Iteration: 1100 Loss (C.E): 0.6304391622543335\n",
            "Epoch: 38/200 Iteration: 1200 Loss (C.E): 0.43913599848747253\n",
            "Epoch: 41/200 Iteration: 1300 Loss (C.E): 0.30777496099472046\n",
            "Epoch: 45/200 Iteration: 1400 Loss (C.E): 0.2341328263282776\n",
            "Epoch: 48/200 Iteration: 1500 Loss (C.E): 0.21265681087970734\n",
            "test set loss value (C.E):  13.988075256347656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3URMAAknUqu",
        "colab_type": "text"
      },
      "source": [
        "### oliver_ch1 as a train set and oliver_test.txt (which is some part of the oliver_ch1) as a test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQwriOgBnAXX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "44be3397-9f39-4310-d46d-8b070f74cff3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "from argparse import Namespace\n",
        "\n",
        "flags = Namespace(\n",
        "    train_file='/content/oliver_ch1.txt',\n",
        "    test_file='/content/oliver_test.txt',\n",
        "    seq_size=32,\n",
        "    batch_size=16,\n",
        "    embedding_size=64,\n",
        "    lstm_size=64, # LSTM hidden size\n",
        "    gradients_norm=5, # norm to clip gradients\n",
        "    initial_words=['I', 'am'], # Initial words to start prediction from\n",
        "    predict_top_k=5, # top k results ro sample word from\n",
        "    # checkpoint_path='checkpoint',\n",
        ")\n",
        "\n",
        "def get_data_from_file(train_file, batch_size, seq_size):\n",
        "    with open(train_file, 'r') as f:\n",
        "        text = f.read()\n",
        "    text = text.split()\n",
        "    # Create two dictionaries, one to convert words into integers indices, \n",
        "    # and the other one to convert integer indices back to word tokens\n",
        "    word_counts = Counter(text)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "    n_vocab = len(int_to_vocab)\n",
        "    print('Vocabulary size', n_vocab)\n",
        "    \n",
        "    # Covert word tokens into integer indices. \n",
        "    # These will be the input to the network\n",
        "    # We will train a mini-batch each iteration \n",
        "    # so we split the data into batches evenly. \n",
        "    # Chopping out the last uneven batch\n",
        "    int_text = [vocab_to_int[w] for w in text]\n",
        "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "    \n",
        "    # In next generation problem, \n",
        "    # the target of each input word will be its consecutive wold,\n",
        "    # so we just shift the whole input data to the left by one step\n",
        "    out_text = np.zeros_like(in_text)\n",
        "    out_text[:-1] = in_text[1:] # in_text의 두번째 부터 out_text의 처음으로 복사\n",
        "    out_text[-1] = in_text[0] # in_text의 처음을 out_text의 마지막으로 복사\n",
        "    in_text = np.reshape(in_text, (batch_size, -1))\n",
        "    out_text = np.reshape(out_text, (batch_size, -1))\n",
        "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text\n",
        "  \n",
        "def get_batches(in_text, out_text, batch_size, seq_size):\n",
        "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
        "    for i in range(0, num_batches * seq_size, seq_size):\n",
        "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]\n",
        "\n",
        "class RNNModule(nn.Module):\n",
        "  def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
        "    super(RNNModule, self).__init__()\n",
        "    self.seq_size = seq_size\n",
        "    self.lstm_size = lstm_size\n",
        "    self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "    self.lstm = nn.LSTM(embedding_size, lstm_size, batch_first=True)\n",
        "    self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "\n",
        "  # Take an input sequence and the previous states (hidden states) and produce the output together with states of the currents timestamp\n",
        "  def forward(self, x, prev_state):\n",
        "    embed = self.embedding(x)\n",
        "    output, state = self.lstm(embed, prev_state)\n",
        "    logits = self.dense(output)\n",
        "    return logits, state # why return state variable?\n",
        "  \n",
        "  # Define one more method to help us set states to zero because we need to reset states at the beginning of every epoch.\n",
        "  def zero_state(self, batch_size):\n",
        "    return (torch.zeros(1, batch_size, self.lstm_size), # hidden state (the short-term memory)\n",
        "            torch.zeros(1, batch_size, self.lstm_size)) # cell state (the long-term memory)\n",
        "\n",
        "def get_loss_and_train_op(net, lr=0.001):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  return criterion, optimizer\n",
        "  # gradient clipping doesn't apply here!\n",
        "\n",
        "def main():\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
        "      flags.train_file, flags.batch_size, flags.seq_size)\n",
        "  test_int_to_vocab, test_vocab_to_int, test_n_vocab, test_in_text, test_out_text = get_data_from_file(\n",
        "      flags.test_file, 1, flags.seq_size)\n",
        "  net = RNNModule(n_vocab, flags.seq_size, flags.embedding_size, flags.lstm_size)\n",
        "  net = net.to(device)\n",
        "  criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
        "  iteration = 0\n",
        "  \n",
        "  # Train\n",
        "  # for each epoch, we will loop through the batches to compute loss valuse and update network's parameters.\n",
        "  for e in range(50):\n",
        "    batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
        "    state_h, state_c = net.zero_state(flags.batch_size)\n",
        "    \n",
        "    # Transfer data to GPU\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    for x, y in batches: # x is in_text and y is out_text\n",
        "      iteration += 1\n",
        "      \n",
        "      # Tell it we are in training mode\n",
        "      net.train()\n",
        "\n",
        "      # Reset all gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Transfer data to GPU\n",
        "      x = torch.tensor(x).to(device)\n",
        "      y = torch.tensor(y).to(device)\n",
        "\n",
        "      logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "      loss = criterion(logits.transpose(1, 2), y) # why we transpose the logits?\n",
        "      \n",
        "      # Avoid autograd which is given by Pytorch to keep track of the tensor's flow to perform back-propagation.\n",
        "      state_h = state_h.detach()\n",
        "      state_c = state_c.detach()\n",
        "\n",
        "      loss_value = loss.item() # this loss is cross-entropy which is thing I want!!! \n",
        "      \n",
        "      # Perform back-propagation\n",
        "      loss.backward()\n",
        "      \n",
        "      # Gradient clipping\n",
        "      _ = torch.nn.utils.clip_grad_norm_(net.parameters(), flags.gradients_norm)\n",
        "\n",
        "\n",
        "      # Update the network's parameters\n",
        "      optimizer.step()\n",
        "\n",
        "      # Print the loss value and have the model generate some text for us during training\n",
        "      if iteration % 100 == 0:\n",
        "        print('Epoch: {}/{}'.format(e, 200), 'Iteration: {}'.format(iteration), 'Loss (C.E): {}'.format(loss_value))\n",
        "\n",
        "      # if iteration % 1000 == 0:\n",
        "      #   predict(device, net, flags.initial_words, n_vocab, vocab_to_int, int_to_vocab, top_k=5)\n",
        "      #   torch.save(net.state_dict(), 'checkpoint_pt/model-{}.pth'.format(iteration))\n",
        "  \n",
        "  # Test\n",
        "  net.eval() # Tell it we are in evaluation mode\n",
        "  # 학습된 parameters을 이용하고 hidden and cell states는 초기화 시켜야함. \n",
        "  state_h, state_c = net.zero_state(1) #\n",
        "  state_h = state_h.to(device) #\n",
        "  state_c = state_c.to(device) #\n",
        "  # Transfer data to GPU\n",
        "  x = torch.tensor(test_in_text).to(device)\n",
        "  y = torch.tensor(test_out_text).to(device)\n",
        "  logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "  loss = criterion(logits.transpose(1, 2), y) # why we transpose the logits?\n",
        "  loss_value = loss.item() # this loss is cross-entropy which is thing I want!!! \n",
        "  print(\"test set loss value (C.E): \", loss_value)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size 4694\n",
            "Vocabulary size 40\n",
            "Epoch: 3/200 Iteration: 100 Loss (C.E): 5.741121292114258\n",
            "Epoch: 6/200 Iteration: 200 Loss (C.E): 4.286619186401367\n",
            "Epoch: 9/200 Iteration: 300 Loss (C.E): 2.989633321762085\n",
            "Epoch: 12/200 Iteration: 400 Loss (C.E): 2.2182981967926025\n",
            "Epoch: 16/200 Iteration: 500 Loss (C.E): 1.6998649835586548\n",
            "Epoch: 19/200 Iteration: 600 Loss (C.E): 1.6040774583816528\n",
            "Epoch: 22/200 Iteration: 700 Loss (C.E): 1.0732179880142212\n",
            "Epoch: 25/200 Iteration: 800 Loss (C.E): 0.8132094740867615\n",
            "Epoch: 29/200 Iteration: 900 Loss (C.E): 0.5646193623542786\n",
            "Epoch: 32/200 Iteration: 1000 Loss (C.E): 0.5946887731552124\n",
            "Epoch: 35/200 Iteration: 1100 Loss (C.E): 0.4816804826259613\n",
            "Epoch: 38/200 Iteration: 1200 Loss (C.E): 0.33147957921028137\n",
            "Epoch: 41/200 Iteration: 1300 Loss (C.E): 0.2688245475292206\n",
            "Epoch: 45/200 Iteration: 1400 Loss (C.E): 0.23053620755672455\n",
            "Epoch: 48/200 Iteration: 1500 Loss (C.E): 0.16258715093135834\n",
            "test set loss value (C.E):  10.106332778930664\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}