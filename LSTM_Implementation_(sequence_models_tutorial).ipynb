{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM Implementation (sequence_models_tutorial)",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPtbqxagfr391+5t1st/xtF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eunjiWon/SoftwareDefectPredictionMetricUsingDeepLearning/blob/master/LSTM_Implementation_(sequence_models_tutorial).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kalrQG6GcedD",
        "colab_type": "text"
      },
      "source": [
        "#SEQUENCE MODELS AND LONG-SHORT TERM MEMORY NETWORKS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSxuH7NSnhpc",
        "colab_type": "text"
      },
      "source": [
        "A recurrent neural network is a network that maintains some kind of state. For example, its output could be used as part of the next input, so that information can propogate along as the network passes over the sequence. In the case of an LSTM, for each element in the sequence, there is a corresponding hidden state h1, which in principle can contain information from arbitrary points earlier in the sequence. We can use the hidden state to predict words in a language model, part-of-speech tags, and a myriad of other things."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3kGtvfepP6j",
        "colab_type": "text"
      },
      "source": [
        "# LSTM's in Pytorch\n",
        "Before getting to the example, note a few things. Pytorch's LSTM expects all of its inputs to be 3D tensors. The semantics of the axes of these tensors is important. The first axis is the sequence itself, the second indexes instances in the mini-batch, and the sequence itself, and the third indexes elements of the input. We haven't discussed mini-batching, so lets just ignore that and assume we will always have just 1 dimension on the second axis. If we want to run the sequence model over the sentence \"The cow jumped\", our input should look like"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HJk0_dpsSD2",
        "colab_type": "text"
      },
      "source": [
        "Let's see a quick example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w68B_BUasWO7",
        "colab_type": "code",
        "outputId": "33550eb4-3cc0-4dd6-b107-e717671c86f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "# Author: Robert Guthrie\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f4ef462aaf0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LboB4TMhspGi",
        "colab_type": "code",
        "outputId": "7745120a-857e-4805-d789-086410e51596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        }
      },
      "source": [
        "lstm = nn.LSTM(3, 3) # Input dim is 3, output dim is 3\n",
        "inputs = [torch.randn(1, 3) for _ in range(5)] # make a sequence of length 5\n",
        "print(inputs) # (1, 3) 사이즈를 가진 tensor가 5개 생김\n",
        "# initialize the hidden state.\n",
        "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))\n",
        "for i in inputs:\n",
        "  # after each step, hidden contains the hidden state.\n",
        "  out, hiddden = lstm(i.view(1, 1, -1), hidden)\n",
        "# initialize the hidden state.\n",
        "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))\n",
        "for i in inputs:\n",
        "  # after each step, hidden contains the hidden state.\n",
        "  out, hiddden = lstm(i.view(1, 1, -1), hidden)\n",
        "print(\"out: \", out)\n",
        "print(\"hidden: \", hidden)\n",
        "\n",
        "# Add the extra 2nd dimension\n",
        "print(\"inputs: \", inputs)\n",
        "inputs = torch.cat(inputs).view(len(inputs), 1, -1) # 5개의 텐서에서 하나의 텐서로 바뀜\n",
        "print(\"inputs: \", inputs)\n",
        "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
        "out, hidden = lstm(inputs, hidden)\n",
        "print(\"out: \", out)\n",
        "print(\"hidden: \", hidden)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([[ 1.7153, -1.1099,  0.3573]]), tensor([[-0.3369, -0.1951,  1.6927]]), tensor([[-0.9122, -0.1971,  0.6831]]), tensor([[ 0.6439, -1.5348,  0.1530]]), tensor([[-0.3657,  0.0245, -0.0813]])]\n",
            "out:  tensor([[[-0.3555,  0.3641, -0.1959]]], grad_fn=<StackBackward>)\n",
            "hidden:  (tensor([[[0.0478, 1.3501, 0.0977]]]), tensor([[[-1.4379,  1.8068, -1.2562]]]))\n",
            "inputs:  [tensor([[ 1.7153, -1.1099,  0.3573]]), tensor([[-0.3369, -0.1951,  1.6927]]), tensor([[-0.9122, -0.1971,  0.6831]]), tensor([[ 0.6439, -1.5348,  0.1530]]), tensor([[-0.3657,  0.0245, -0.0813]])]\n",
            "inputs:  tensor([[[ 1.7153, -1.1099,  0.3573]],\n",
            "\n",
            "        [[-0.3369, -0.1951,  1.6927]],\n",
            "\n",
            "        [[-0.9122, -0.1971,  0.6831]],\n",
            "\n",
            "        [[ 0.6439, -1.5348,  0.1530]],\n",
            "\n",
            "        [[-0.3657,  0.0245, -0.0813]]])\n",
            "out:  tensor([[[ 0.3059,  0.3641,  0.0463]],\n",
            "\n",
            "        [[ 0.0652,  0.2791, -0.1788]],\n",
            "\n",
            "        [[-0.1173,  0.2761, -0.0856]],\n",
            "\n",
            "        [[-0.3872,  0.3872, -0.2095]],\n",
            "\n",
            "        [[-0.3846,  0.2223, -0.1080]]], grad_fn=<StackBackward>)\n",
            "hidden:  (tensor([[[-0.3846,  0.2223, -0.1080]]], grad_fn=<StackBackward>), tensor([[[-0.9864,  0.3858, -0.2158]]], grad_fn=<StackBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhYjYvHWLDhN",
        "colab_type": "text"
      },
      "source": [
        "###view function\n",
        "```\n",
        "x = torch.randn(4, 4)\n",
        "x.size()\n",
        "y = x.view(16)\n",
        "y.size()\n",
        "z = x.view(-1, 8)\n",
        "z.size()\n",
        "x.size()\n",
        "```\n",
        "inputs의 size는 torch.Size([5, 1, 3]) 이다.\n",
        "```\n",
        "for i in inputs:\n",
        "  i.view(1, 1, -1)\n",
        "```\n",
        "위에 처럼 해주면 i의 size는 각각 torch.Size([1, 1, 3])이 된다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G01kV72XVD1V",
        "colab_type": "text"
      },
      "source": [
        "# Example: An LSTM for Part-of-Speech Tagging\n",
        "In this section, we will use an LSTM to get part of speech tags. (참고로 POS tagging은 문장 내 단어들의 품사를 식별하여 태그를 붙여주는 것을 말한다. tuple의 형태로 출력되며 (단어, 태그)로 출력된다.) We will not use Viterbi (비터비 알고리즘은 히든 스테이트의 최적 시퀀스를 찾기 위한 다이나믹 프로그래밍 기법의 일종임) or Forward-Backward of anything like that.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sicwRzoEVRa_",
        "colab_type": "code",
        "outputId": "82518777-2159-47da-a867-0f93acc80a1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "# Prepare data\n",
        "def prepare_sequence(seq, to_ix): # turn seq into tensors of word indices\n",
        "  idxs = [to_ix[w] for w in seq]\n",
        "  print(\"idxs: \", idxs) # e.g., \"idxs: [5, 6, 7, 8]\"\n",
        "  return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "training_data = [\n",
        "                 (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "                 (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
        "]\n",
        "word_to_ix = {}\n",
        "for sent, tags in training_data:\n",
        "  for word in sent:\n",
        "    if word not in word_to_ix:\n",
        "      word_to_ix[word] = len(word_to_ix) # index를 부여하기 위해 word_to_ix의 사이즈를 이용\n",
        "# print(\"word_to_ix: \", word_to_ix)\n",
        "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2} # manually indexing\n",
        "# print(\"tag_to_ix: \", tag_to_ix)\n",
        "\n",
        "# These will usually be more like 32 or 64 dimensional. ???\n",
        "# We will keep them small. so we can see how the weights change as we train. ???\n",
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6 "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word_to_ix:  {'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n",
            "tag_to_ix:  {'DET': 0, 'NN': 1, 'V': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTPje7wrVRzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the model\n",
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "  def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "    super(LSTMTagger, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # The LSTM takes word embeddings as inputs, and outputs hidden states with dimensionality hidden_dim.\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "    # The linear layer that maps from hidden state space to tag space\n",
        "    # tag가 output이라서 \n",
        "    self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "  \n",
        "  def forward(self, sentence):\n",
        "    # print(\"sentence size: \", sentence.size())\n",
        "    embeds = self.word_embeddings(sentence) # here is an actual embedding execution \n",
        "    lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1)) # 하나의 텐서로 만들어 주는 듯...\n",
        "    # print(\"lstm_out.view(len(sentence), -1): \", lstm_out.view(len(sentence), -1))\n",
        "    tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1)) # target_size parameter는 어디서 넘겨주는거지?\n",
        "    tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "    # print(\"tag_scores: \", tag_scores)\n",
        "    return tag_scores\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CdW_MibVSG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1) # SGD는 학습데이터 1개마다 가중치를 업데이트하기 때문에 전체 학습테이터가 N개면 epoch이 N번\n",
        "\n",
        "# See what the scores are before training\n",
        "# Note that element i, j of the output is the score for tag j for word i.\n",
        "# Here we don't need to train, so the code is wrapped in torch.no_grad() \n",
        "# 왜 train 필요없는거지지? 간단해서???\n",
        "with torch.no_grad(): # 또한 with torch.no_grad(): 로 코드 블럭을 감싸서 autograd가 .requires_grad=True 인 Tensor들의 연산 기록을 추적하는 것을 멈출 수 있습니다.\n",
        "  inputs = prepare_sequence(training_data[0][0], word_to_ix) # make a tensor\n",
        "  tag_scores = model(inputs) # 알아서 forward 함수가 실행되는건가보다...\n",
        "  print(\"Before training tag_scores: \", tag_scores)\n",
        "\n",
        "for epoch in range(300): # again, normally you would NOT do 300 epochs, it is toy data\n",
        "  for sentence, tags in training_data:\n",
        "    # Step 1. Remember that Pytorch accumulates gradients.\n",
        "    # We need to clear them out before each instance\n",
        "    model.zero_grad()\n",
        "    # Step 2. Get our inputs ready for the network, that is, turn them into tensors of word indices.\n",
        "    sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "    targets = prepare_sequence(tags, tag_to_ix) # label\n",
        "    # Step 3. Run our forward pass.\n",
        "    tag_scores = model(sentence_in)\n",
        "    # Step 4. Compute the loss, gradients, and update the parameters by calling optimizer.step()\n",
        "    loss = loss_function(tag_scores, targets) # 위에서 nn.NLLLoss()로 정의했음 \n",
        "    # test 할 때도도 loss를 계산할 수 있겠지?\n",
        "    # print(\"training... loss: \", loss)\n",
        "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
        "    # parameters of the model. Internally, the parameters of each Module are stored\n",
        "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
        "    # all learnable parameters in the model.\n",
        "    loss.backward()  \n",
        "    optimizer.step() # parameters을 업데이트함\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad71LDnPFvdJ",
        "colab_type": "code",
        "outputId": "344a816c-1925-44ba-ed01-a3b9b5c0ae3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "# See what the scores are after training\n",
        "with torch.no_grad():\n",
        "  inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "  print(inputs)\n",
        "  tag_scores = model(inputs)\n",
        "  targets = prepare_sequence(training_data[0][1], tag_to_ix) # label\n",
        "  print(targets)\n",
        "  loss = loss_function(tag_scores, targets)\n",
        "  print(\"After training tag_scores: \", tag_scores)\n",
        "  print(\"After training loss: \", loss)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "idxs:  [0, 1, 2, 3, 4]\n",
            "tensor([0, 1, 2, 3, 4])\n",
            "idxs:  [0, 1, 2, 0, 1]\n",
            "tensor([0, 1, 2, 0, 1])\n",
            "After training tag_scores:  tensor([[-0.1047, -2.9359, -3.0729],\n",
            "        [-3.3204, -0.0398, -5.8491],\n",
            "        [-2.9641, -4.9341, -0.0606],\n",
            "        [-0.0447, -3.9176, -3.7365],\n",
            "        [-3.8830, -0.0220, -6.7731]])\n",
            "After training loss:  tensor(0.0544)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}